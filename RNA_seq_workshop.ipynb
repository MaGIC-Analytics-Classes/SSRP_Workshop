{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to RNA Sequencing\n",
    "Welcome to the RNA-sequencing workshop for SSRP. This notebook will cover step-by-step how to perform a basic RNA-seq experiment. \n",
    "\n",
    "As this is built to operate on our [Binderhub](https://binderhub.readthedocs.io/en/latest/index.html), the environment has been preconfigured with appropriate software installed. These softwares will be called as needed. To view them, please see the environment file within the binder directory. If you try to run this notebook on a local machine without properly installing each software package, it will not operate correctly. \n",
    "\n",
    "Since this Jupyter kernel is based on Python, the base code for each block is Python unless defined otherwise. Text will be generated in markdown blocks, but otherwise look for %% to then define the language for the block. Such as a block starting with \"%%bash\", it will be executed as if it were the standard command line interface terminal, or \"%%R\", executes as if it were R code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primary analysis\n",
    "Primary analysis entails the first line of analysis, in this case the very first step will have already been performed by your sequencing service provider- demultiplexing. This step converts the native Illumina base calls into the FASTQ format, while separating out different samples based upon their respective index. Per sample you should have:\n",
    "- SampleName_R1.fastq.gz\n",
    "- SampleName_R2.fastq.gz\n",
    "\n",
    "The .gz at the end is a [gzip](https://www.gzip.org/) compression to reduce the overall file size. \n",
    "\n",
    "In this case, we will be utilizing publically available data. To that end, we would traditionally need to download the files through the [Short Reads Archive(SRA)](https://www.ncbi.nlm.nih.gov/sra). This would involve using their SRA toolkit, which has already been installed in this environment and used to download the files. The code would look like:\n",
    "```bash\n",
    "fastq-dump --split-files --gzip --outdir rawfiles/ SAMN09354753\n",
    "```\n",
    "Which would need to be done for each sample. The --split-files generates both R1 and R2, --gzip allows for continued compression, and --outdir notes the output directory for the files. \n",
    "\n",
    "Unfortunately, the SRA toolkit can be rather finnicky and doesnt like to operate consistently. In some cases it can be easier to pull the files from the [European Nucleotide Archive (ENA)](https://www.ebi.ac.uk/ena). For this, you need to find the FTP path, but then you can just use the standard unix tool wget to download the files. \n",
    "```bash\n",
    "wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR726/008/SRR7261718/SRR7261718_1.fastq.gz\n",
    "```\n",
    "For expediency- this has been included during the notebook initialization, and all samples should be renamed to fit the appropriate name below. Each sample should have the name followed by an underscore and then a 1 or 2. The 1 or 2 denotes if it was Read1 or Read2. \n",
    "\n",
    "For this workshop, we will be using data from [GSE115330](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE115330). This study was done to investigate the effects of various pollutions on the transcriptome of *Escherichia coli*. Each sample's RNA was isolated via trizol extraction, followed by ribosomal depeletion with the RiboZero kit, libraries generated by the TruSeq RNA library kit (sonic fragmentation), and lastly sequenced on an Illumina HiSeq 4000 in a 2x150bp configuration. \n",
    "\n",
    "**Sample breakdown table**\n",
    "\n",
    "|  SRA Sample Name | SRA Run Name | Sample Name | Sample Type |\n",
    "| --- | --- | --- | --- |\n",
    "| SAMN09354753 | SRR7261718 | WT1 | Wild type |\n",
    "| SAMN09354752 | SRR7261719 | WT2 | Wild type |\n",
    "| SAMN09354751 | SRR7261720 | WT3 | Wild type |\n",
    "| SAMN09354750 | SRR7261721 | Urban1 | Treated with collected urban air |\n",
    "| SAMN09354749 | SRR7261722 | Urban2 | Treated with collected urban air |\n",
    "| SAMN09354748 | SRR7261723 | Urban3 | Treated with collected urban air |\n",
    "| SAMN09354747 | SRR7261724 | Diesel1 | Treated with collected diesel exhaust |\n",
    "| SAMN09354746 | SRR7261725 | Diesel2 | Treated with collected diesel exhaust |\n",
    "| SAMN09354745 | SRR7261726 | Diesel3 | Treated with collected diesel exhaust |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After receiving these files from your sequencing service provider (or in this case, downloading them), the next step in primary analysis is checking the quality of the reads. This follows the simple principle of **Garbage in=Garbage out** and always drives the need for the highest quality input available.\n",
    "\n",
    "### FASTQ quality check\n",
    "To check your FASTQ files, we will use FASTQC which is published by the [Babraham Bioinformatics](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/) team as the first line QC tool.\n",
    "\n",
    "This tool will evaluate the overall health of your sample. In particular, you will want to look at:\n",
    "- Total number of sequences\n",
    "    - Make sure it aligns with your targetted expectations\n",
    "- Per base sequencing quality\n",
    "    - It should drop over time, especially in 2x300bp reads. Ensure that it doesnt drop too far, usually you want Q30>90% if possible\n",
    "- QC content\n",
    "    - Make sure it aligns with your genome of interest\n",
    "- %N\n",
    "    - N is an ambiguous base. You dont want many of these in your sample (if any)\n",
    "- Sequence Length\n",
    "    - Make sure the length you put in to the sequencer is whats coming out\n",
    "- Duplication levels\n",
    "    - Keep as low as possible, but any library with PCR you expect some\n",
    "- Adapter content\n",
    "    - This will be driven by your insert size and sequencing length. If you are sequencing through your entire insert and in to the adapter on the other side, you would expect contamination here. Usually best to avoid that, but if you see some you will need to trim it out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "for file in raw_files/*.fastq.gz\n",
    "do \n",
    "    fastqc $file &\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if you didnt want to do that manually, you could set up a loop to iterate through and do this on all files that end in \".fastq.gz\" as Ive done. As an example, here is a small code snippet to show how it would be done indivudally. Try it out if you would like!\n",
    "\n",
    "```bash\n",
    "fastqc raw_files/WT1_1.fastq.gz\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view your FASTQC output, navigate into the raw_align directory (containing the raw files for alignment) and look for .html files that are the output. Download those and then open them in your browser to explore the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Trimming\n",
    "After checking the input quality of the samples, you often (or always) will need to trim the samples. This can be critical because again, **Garbage in=Garbage out**. \n",
    "This step remove ambiguous bases, low quality bases/reads, and also removes any adapter content that may be in the read. For most trimming cases, [Trimmomatic](http://www.usadellab.org/cms/?page=trimmomatic) is a viable tool. Another commonly used tool is [CutAdapt](https://cutadapt.readthedocs.io/en/stable/), but here we will use trimmomatic. \n",
    "\n",
    "Parameters used:\n",
    "- PE\n",
    "    - Denotes the input as paired end reads. Following that, you need to put the two input files, then trimmed-1-unpaired, trimmed-1-paired, trimmed-2-unpaired, trimmed-2-paired for output files\n",
    "- ILLUMINACLIP\n",
    "    - To clip off specifically any Illumina reads that are found. The numbers that follow are the seed mismatch (maximum count of mismatches to identify adapter), palindrome clip threshold (to remove possible identical adapters on both ends of reads), and the simple clip threshold (how accurate the adapter is to read beyond the seed)\n",
    "- LEADING TRAILING\n",
    "    - Removes bases from the start and end of the read if they are below thresholds. \"2\" is Illumina for \"low quality\"\n",
    "- SLIDING WINDOW\n",
    "    - Looks at the read in a sliding window frame and takes the average of multiple bases. The first number is the window size and the second is quality score. Again \"2\" is Illumina for \"low quality\", but this allows for 1 base to not potentially trash an otherwise high quality read. \n",
    "- MINLEN\n",
    "    - The minimum length of a read after trimming to accept.  \n",
    "\n",
    "In this case, I opted to use a quick bash loop. The first line looks through the raw files directory, then pulls out the sample name from the fastq files. We then can use that to build the aspects of the trimmomatic command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir trimmed\n",
    "for prefix in $(ls raw_files/*.fastq.gz | sed -r 's/_[12][.]fastq.gz//' | uniq)\n",
    "do\n",
    "    output=$(echo $prefix | cut -d'/' -f2-)\n",
    "    trimmomatic PE ${prefix}_1.fastq.gz ${prefix}_2.fastq.gz \\\n",
    "    trimmed/${output}_trimmed_1_paired.fastq.gz trimmed/${output}_trimmed_1_unpaired.fastq.gz \\\n",
    "    trimmed/${output}_trimmed_2_paired.fastq.gz trimmed/${output}_trimmed_2_unpaired.fastq.gz \\\n",
    "    ILLUMINACLIP:raw_files/adapters.fasta:2:40:15 \\\n",
    "    LEADING:2 TRAILING:2 \\\n",
    "    SLIDINGWINDOW:4:2 \\\n",
    "    MINLEN:140 &\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then do a QC check on the samples again to see how they are after trimming!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "for file in trimmed/*.fastq.gz\n",
    "do \n",
    "    fastqc $file &\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And with that you have checked the quality of your samples, removed low quality contaminants, and can show the hopefully high-quality input for your secondary analysis!\n",
    "We also introduce a tool called [MultiQC](https://multiqc.info/). This is a great QC aggregation tool. Play around with it a bit, but after running this you can download the multiqc file, open it in a browser, and view things aggregated instead of all the individual files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "multiqc trimmed/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Secondary analysis\n",
    "So secondary analysis is the data reduction step of an NGS workflow. Specifically to RNA-sequencing, this is where we would align the reads to the reference and assess the quality of alignment. There are many different aligners out there, but the one that has been the gold standard for RNA-sequencing has been [STAR](https://github.com/alexdobin/STAR). This aligner is accurate, fast, and splice-aware (for applicable organisms). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build STAR reference index\n",
    "The first step in performing STAR alignment is generating a reference index. The STAR algorithm uses a seed searching technique on an uncompressed suffix array, so requires indexing. \n",
    "\n",
    "During the notebook initilization, the reference genome and transcript tracks should have been downloaded as well into the references directory. For reference, we will be using the [K-12 strain of *E.coli*](http://bacteria.ensembl.org/Escherichia_coli_str_k_12_substr_mg1655/Info/Index)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir star\n",
    "STAR --runMode genomeGenerate \\\n",
    "    --sjdbGTFfile references/ecoli_genes.gtf \\\n",
    "    --sjdbGTFtagExonParentTranscript Parent \\\n",
    "    --genomeSAindexNbases 10 \\\n",
    "    --genomeDir star/ \\\n",
    "    --genomeFastaFiles references/ecoli.fasta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Align reads to reference\n",
    "Now that your reference is built appropriately for STAR, you can align your reads to it. This will be the longest steps of the processing. Below is a standard command for calling STAR. There are a variety of additional parameters you can pass to tweak the operations. These would generally be more advanced applications, and involve altering the seeding/extensions/scoring. For here though, we are going to make a directory for the output, pull our sample names again into a loop, and align each sample. The last step is to index the output file as well- this makes it easier to import into visualizing tools. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir aligned\n",
    "for prefix in $(ls trimmed/*.fastq.gz | sed -r 's/_[12]_paired[.]fastq.gz//' | uniq)\n",
    "do\n",
    "    output=$(echo $prefix | cut -d'/' -f2-)\n",
    "    STAR --genomeDir star/ \\\n",
    "            --sjdbGTFfile references/ecoli_genes.gtf \\\n",
    "            --readFilesIn ${prefix}_1_paired.fastq.gz ${prefix}_2_paired.fastq.gz \\\n",
    "            --twopassMode Basic \\\n",
    "            --outWigType bedGraph \\\n",
    "            --outSAMtype BAM SortedByCoordinate \\\n",
    "            --readFilesCommand zcat \\\n",
    "            --outFileNamePrefix aligned/${output}\n",
    "    samtools index aligned/${output}Aligned.sortedByCoord.out.bam\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post alignment QC\n",
    "Now as has been mentioned several times, Garbage in=Garbage out. This is where we can more in depth evaluate the output quality to ensure that the different samples were successful throughout the entire workflow. We will use several tools for generating different aspects of QC, and then pull them together using the MultiQC tool. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RSeqQC\n",
    "The first tool we will use for the post run QC is called [RSeqQC](http://rseqc.sourceforge.net/). We will use several of its scripts to measure different aspects of our aligned data. You'll notice each of these scripts ends in \".py\". This notes it as a python script- this program is built in python. \n",
    "- bam_stat.py: This script pulls the basic mapping statistics from the aligned file. This is the information like number of mapped reads, which were in proper pairs, which spanned splice junctions etc\n",
    "- inner_distance.py: This script attempts to calculate the average inner distance of your fragments. This is the gap space between your reads. If your reads overlap, it should be zero. If you had say 400 bp fragments and did 2x100bp sequencing, you would expect an inner distance of ~200bp.  \n",
    "- read_distribution.py: This script takes a look at where your reads were mapping. Was the read in an exonic region, intronic etc. \n",
    "- read_duplication.py: This script looks at the duplication of reads. In most cases of RNA-seq since there is amplification there is some expected duplication. If it is too high though, this means you probably overamplified and ended up sequencing the same tiny fragments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir out_qc\n",
    "for file in $(ls aligned/*_trimmedAligned.sortedByCoord.out.bam)\n",
    "do\n",
    "    name=$(echo $file | cut -d'/' -f2-)\n",
    "    bam_stat.py -i $file 2> out_qc/${name}.bam_stat.txt\n",
    "    inner_distance.py -i $file -o out_qc/${name}.rseqc -r $bed12\n",
    "    read_distribution.py -i $file -r ${name} > out_qc/${name}.read_distribution.txt\n",
    "    read_duplication.py -i $file -o out_qc/${name}.read_duplication\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FeatureCounts\n",
    "The next tool we will use is called [featureCounts](http://subread.sourceforge.net/), part of the Subread package. Similar to calculating the read distribution above, it also counts the reads that are aligned to each region. This gives us the raw hit counts data we need to perform the differential gene expression analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "for file in $(ls aligned/*_trimmedAligned.sortedByCoord.out.bam)\n",
    "do\n",
    "    name=$(echo $file | cut -d'/' -f2-)\n",
    "    featureCounts -a references/ecoli_genes.gtf -g 'gene_id' -t 'exon' -o counts/${name}_gene.featureCounts.txt --extraAttributes 'gene_name' -p $file\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you'll have a bunch of individual featureCount files. At this point you'll need to merge them together to be able to perform downstream workflows. You can do this merging a variety of ways, using a python loop, some R. I opted in this case to use a quick bit of bash below. It navigates into the counts directory we made above, lists all of the files so it can then \"clean\" them up to be just the values for each gene, gets the gene names, then pastes it all together. The output is then a tab delimited spreadsheet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd counts\n",
    "ls -1 *featureCounts.txt | parallel 'cat {} | sed '1d' | cut -f8 {} > {/.}_clean.txt'\n",
    "ls -1 *featureCounts.txt | head -1 | xargs cut -f1 > genes.txt\n",
    "paste genes.txt *featureCounts_clean.txt > merged_gene_counts.txt\n",
    "sed -i 1d merged_gene_counts.txt\n",
    "cd ..\n",
    "#Sometimes you may have the massage the data a bit at this point to get it to correctly enter the DESeq portions.\n",
    "#I may have a typo or bug somewhere in here, so if it isnt perfect dont be alarmed- mistakes can happen. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Here for the quick version!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tertiary analysis\n",
    "Now that we have determined we have a quality mapping and we have our table of gene counts, we are ready for the next step- differential gene expression analysis. One of the most prolificly utilized is [DESeq2](https://bioconductor.org/packages/release/bioc/html/DESeq2.html). \n",
    "\n",
    "For this workshop, Ive included the feature counts file we would have generated above to save time, as it can take more time than we have to run the above steps. If there are any bugs in the above steps as well, then this will circumvent that as you now will have the appropriate files for downstream. \n",
    "\n",
    "Specifically for RNA-seq experiments like this, we need to take into account several aspects of the biology at play for the statistics. Primarily, a standard t-test assumes a normal distribution. Like a coin flip- If you flip a coin 50 times per test and do 50 test, you would expect the brunt of the output to be near the 25 heads/25 tails, with it being less and less likely to get 1 heads/49 tails or 1 tails/49 heads. That would be a normal distribution. With RNA-seq, we expect to see thousands of \"tests\" (genes) with many of them having few coin flips (reads) and then a select few having a larger portion of reads. This is more comparable to the lottery, many many will play, some will win a little bit, and very few will hit the jackpot. This type of distribution is usually called a Poisson distribution. RNA-seq goes even a bit further and no longer assumes that the mean and variance will be the same, and instead assumes that the variance is independent of the mean. This is called negative binomial distribution, and as you can see below fits an RNA-seq distribution. \n",
    "\n",
    "![https://biohpc.cornell.edu/doc/RNA-Seq-2019-Lecture3.pdf](img/nb_mean_var.png)\n",
    "\n",
    "You also have to consider how to normalize your data. This is critical in RNA-seq data for the following effects:\n",
    "- Significance due to different sequencing depth. If Sample A is twice as deep as Sample B, well then the genes will look twice as expressed\n",
    "- Significance due to gene size. If Gene X is twice as long as Gene Y, you would expect more reads to map to Gene X since more of the original RNA would come from the larger gene, and therefore Gene X will look more expressed than Gene Y\n",
    "- Significance due to RNA depth/composition. There will be several genes that are the most expressed based on the negative binomial distribution. These \"super signals\" will reduce the visibility of other signals. I always think of a car with those stupid bright HID LED lights driving at night looks like the light of the sun, and then the next car with normal headlights looks like its lights arent even on. Similar concept. \n",
    "\n",
    "With DESeq2, we fortunately can take into account these normalizations which we will go into a bit more below. \n",
    "\n",
    "Here, we will begin to use a different language than the bash above, and instead will use R for performing the stats and basic visualization. You can see each block will be run as R by the \"%%R\" that starts the block, like the \"%%bash\" noted bash above. \n",
    "\n",
    "The first step is loading the DESeq2 library and loading in the raw data. For this, we have pre-built the metadata table saying what each sample is and the comparators. During the first bit we will load all three samples, then we will walk back a few steps and repeat to perform the appropriate pairwise comparisons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo \",SampleName,Type\" >> meta.csv\n",
    "echo \"WT1,WT1,WildType\" >> meta.csv\n",
    "echo \"WT2,WT2,WildType\" >> meta.csv\n",
    "echo \"WT3,WT3,WildType\" >> meta.csv\n",
    "echo \"Urban1,Urban1,Urban\" >> meta.csv\n",
    "echo \"Urban2,Urban2,Urban\" >> meta.csv\n",
    "echo \"Urban3,Urban3,Urban\" >> meta.csv\n",
    "echo \"Diesel1,Diesel1,Diesel\" >> meta.csv\n",
    "echo \"Diesel2,Diesel2,Diesel\" >> meta.csv\n",
    "echo \"Diesel3,Diesel3,Diesel\" >> meta.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rpy2.robjects as robjects\n",
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "#load the library for DESeq2\n",
    "suppressMessages(library(DESeq2)) \n",
    "\n",
    "#Set up your parameters for this, in this case a path to your counts, your metadata, and which column in the metadata for cross comparisons. \n",
    "params <- list(feature_counts='counts/merged_gene_counts.txt', \n",
    "               annotation='meta.csv',\n",
    "               condition='Type')\n",
    "\n",
    "#Load your count file, it should be a tab delimited text file with headers. \n",
    "counts <- read.csv(params$feature_counts,\n",
    "                       sep              = \"\\t\",\n",
    "                       header           = TRUE, \n",
    "                       stringsAsFactors = FALSE, \n",
    "                       check.names      = FALSE);\n",
    "\n",
    "#Get the rownames, order them, and shift it over\n",
    "rownames(counts) <- counts[,1]\n",
    "counts <- counts[, -c(1:2)]\n",
    "counts <- counts[ , order(names(counts))]\n",
    "#Load in your metadata and likewise order it all to pair sample names in the metadata to the names in the counts file\n",
    "sampleTable <- read.csv(params$annotation, row.names = 1)\n",
    "sampleTable <- sampleTable[ order(row.names(sampleTable)), ]\n",
    "countdata <- as.matrix(counts)[, colnames(counts) %in% rownames(sampleTable)]\n",
    "condition <- unlist(sampleTable[params$condition])\n",
    "\n",
    "#load a DESeq Data Set (dds for variable name) based on the inputs above. \n",
    "dds <- DESeqDataSetFromMatrix(countData=countdata, colData=sampleTable, design= as.formula(paste(\"~\", params$condition)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've loaded your data and got it inputted correctly, you need to consider how DESeq2 performs differential gene expression analysis and the normalization mentioned above. You will need to:\n",
    "- Model the raw counts for each gene\n",
    "    - Estimate size factors. Normalizes the data using a median of ratios method\n",
    "    - Estimate gene-wise dispersion. The spread of variability in the data. \n",
    "    - Fit and shrink to gene-wise dispersion. For the RNA-depth/composition comparison, assumes that similar expression levels have similar dispersion and can be shrunk together to the generalized fit.\n",
    "    - Generalized Linear Model fit for each gene\n",
    "- Shrink log2 fold changes if applicable. This wont change the output stats, but will reduce the variance of the samples to the means of the gene, and can make for different visualizations.\n",
    "- Statistically test for differential gene expression (more detail below)\n",
    "\n",
    "As briefly touched on above, this will help you normalize the data to take into account sequencing depth, gene size, and RNA depth/composition. All together, this starts to look like:\n",
    "![https://github.com/hbctraining/DGE_workshop_salmon/blob/master/img/deseq_dispersion2.png](img/deseq_dispersion2.png)\n",
    "\n",
    "You can see how it brings the normalized counts onto a properly fit dispersion pattern. \n",
    "\n",
    "Now for the statistics, DESeq2 traditionally uses the Wald test. The Wald test specifically takes into account the dispersion model distance and the negative binomial distribution that we experience with RNA-seq. We then assume our null hypothesis is that there is no differential gene expression between the groups, and p is small, we can reject the null and say the gene is differentially expressed. One last thing to consider with the p-value is multiple testing correction. Traditionally p < 0.05 is considered significant (95% chance it is not by random chance), but when you are looking at thousands of genes, you'll still end up with 5% of them significant off predicted chance. One of the best corrections to use called False-Discovery Rate correction (FDR), which is the Benjamini and Hochberg method. This corrects the p-value based upon ranking the genes and accounting for the number of genes in the dataset. If you want to be even more conservative, you can use a bonferroni correction, but here we use FDR.\n",
    "\n",
    "In the end one of the great things about DESeq2, is that it covers each of these aspects and if you manually want to adjust them you can, but it will automatically perform each of these steps with a simple command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "dds <- DESeq(dds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yup, as simple as that to start off. Now lets take a look at the dispersion we see in our samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "plotDispEsts(dds, main=\"Dispersion plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now at this step it also is good practice to look at how related samples are, which can also act as a mild QC to check that there werent any obvious sample mix ups. In this case, we will do a rlog transformation on the data for visualization purposes, and then we will do a sample relationship distance matrix and PCA plot. This code will be shown and lightly annotated. In R and Python, the \"#\" notes an annotation (or comment). Anything after \"#\" will not be run as code in that line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "rld <- rlogTransformation(dds)\n",
    "suppressMessages(library(RColorBrewer)) #An R library to add some fancy colors\n",
    "suppressMessages(library(gplots))\n",
    "(mycols <- brewer.pal(8, \"Dark2\")[1:length(unique(condition))]) #Setting colors to the samples\n",
    "sampleDists <- as.matrix(dist(t(assay(rld)))) #Calculating the distance between each sample as a matrix\n",
    "heatmap.2(as.matrix(sampleDists), key=F, trace=\"none\",\n",
    "          col=colorpanel(100, \"black\", \"white\"),\n",
    "          ColSideColors=mycols[condition], RowSideColors=mycols[condition],\n",
    "         main=\"Day 14 Distance Matrix\",\n",
    "         margins=c(12,12),\n",
    "         srtCol=45,\n",
    "         srtRow=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "suppressMessages(library(ggplot2)); suppressMessages(library(ggrepel)); suppressMessages(library(dplyr))\n",
    "rv = rowVars(assay(rld)) #Assign the row variables\n",
    "select = order(rv, decreasing = TRUE)[seq_len(min(500, length(rv)))] #select the top facets for PCA\n",
    "pca = prcomp(t(assay(rld)[select, ])) #execute PCA. This can be done with a few methods that are explored more in depth for a full course\n",
    "data <- plotPCA(rld, intgroup=params$condition, returnData=TRUE) #generate the initial PCA plot\n",
    "pc1var <- round(summary(pca)$importance[2,1]*100, digits=1) #Set the variable numbers for the label\n",
    "pc2var <- round(summary(pca)$importance[2,2]*100, digits=1)\n",
    "pc1lab <- paste0(\"PC1 (\",as.character(pc1var),\"%)\") #and now make the label\n",
    "pc2lab <- paste0(\"PC1 (\",as.character(pc2var),\"%)\")\n",
    "#all the ggplot details to add labels and make it look pretty. \n",
    "p1 <- ggplot(data, aes(PC1, PC2))\n",
    "p1 <- p1 + geom_point(aes(color=mycols[condition], shape=mycols[condition]),alpha=0.55,size=5)\n",
    "p1 <- p1 + scale_shape_manual(values=c(15, 17, 19, 18))\n",
    "p1 <- p1 + xlab(pc1lab)\n",
    "p1 <- p1 + ylab(pc2lab)\n",
    "p1 <- p1 + geom_text_repel(aes(label=rownames(data)))\n",
    "p1 <- p1 + theme(legend.position = \"none\")\n",
    "print(p1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now take a look at how related the samples are in the plots above. Each of the samples per group should be fairly close together, and you can see variance from group to group. \n",
    "\n",
    "The last thing we will do here is organize the output tables to show the appropriate cross comparisons. Going back to the biology, we will do the different conditions vs the wild type. Since this is for visualization purposes as well, we will implement the log fold change shrinkage mentioned above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "#Set up comparisons based on the metadata table\n",
    "contrast_urban <- c('Type', 'Urban','WildType')\n",
    "contrast_diesel <- c('Type','Diesel','WildType')\n",
    "\n",
    "#Pull the results from the dataset and the appropriate contrasts\n",
    "result_table_urban_preshrink <- results(dds, contrast=contrast_urban, alpha=0.05)\n",
    "result_table_diesel_preshrink <- results(dds, contrast=contrast_diesel, alpha=0.05)\n",
    "\n",
    "#Perform the lfc shrinkage. \n",
    "urban_out <- lfcShrink(dds, contrast=contrast_urban, res=result_table_urban_preshrink)\n",
    "diesel_out <- lfcShrink(dds, contrast=contrast_diesel, res=result_table_diesel_preshrink)\n",
    "\n",
    "#Make a quick directory to store the output files\n",
    "dir.create('./DEG/')#DEG stands for differentially expressed genes\n",
    "\n",
    "#Write it out as a csv file for your later pleasure\n",
    "write.csv(as.data.frame(urban_out), file='./DEG/urban_output.csv')\n",
    "write.csv(as.data.frame(diesel_out), file='./DEG/diesel_output.csv')\n",
    "write.csv(as.data.frame(counts(dds, normalized=TRUE)), file='./DEG/full_counts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quartenary analysis\n",
    "Now the last stages of analysis are also the most open ended- and this is considered quartenary analysis. This is where you now start to connect all the hard work you've done into some biological conclusions. In a lot of cases you'll already have some aspect you are focusing in on, maybe like the role of TH1 genes in your immune reponse to drug X or something, but other times you are purely on a fishing expedition. So here Ill leave it a bit more open ended- how would you start to explore this data? \n",
    "\n",
    "You could look at specific genes of interest, look at just the top genes, start plotting them out in different fashions. Ill give a few examples below, but then start to explore the data and come up with some biological conclusions. \n",
    "\n",
    "**Spoiler alert**, this paper was already published so if you really come up dry, go look there for inspiration. \n",
    "\n",
    "### Basic Plotting Options\n",
    "Here Ill give some quick demonstrations of common plotting methods and things you might look at. \n",
    "\n",
    "#### Volcano plots\n",
    "Volcano plots are a staple in RNA-seq analysis. These are generally used to look at the spread of differentially expressed genes between two samples. They are usually structured to show the high count of insignificant genes in on the bottom, then the more significant and alternatively regulated genes exploding out- looking a bit like a [plinian volcanic eruption](https://www.youtube.com/watch?v=VBTAcACmcgo) (hence the name- volcano plots). \n",
    "\n",
    "You can create an quick and dirty volcano plot using the code below, but if you want to get fancy with it I would recommend the [EnhancedVolcano](https://bioconductor.org/packages/release/bioc/html/EnhancedVolcano.html) package. If you want to get real hardcore, combine it with [plotly](https://plotly.com/) for interactive plots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "res <- read.csv(\"./DEG/urban_output.csv\", header=TRUE, row.names=1)\n",
    "\n",
    "# Make a basic volcano plot\n",
    "with(res, plot(log2FoldChange, -log10(pvalue), pch=20, \n",
    "               main=\"Urban Atmosphere vs Standard Laboratory Atmosphere\", \n",
    "               xlim=c(-3,3), \n",
    "               ylim=c(0,40),cex=0.5))#change xlim or ylim to frame it out. cex is dot size  \n",
    "\n",
    "# Add colored points: red if padj<0.05, orange of log2FC>1, green if both)\n",
    "with(subset(res, padj<.05 ), points(log2FoldChange, -log10(pvalue), pch=20, col=\"red\",cex=0.5))\n",
    "with(subset(res, abs(log2FoldChange)>1), points(log2FoldChange, -log10(pvalue), pch=20, col=\"orange\",cex=0.5))\n",
    "with(subset(res, padj<.05 & abs(log2FoldChange)>1), points(log2FoldChange, -log10(pvalue), pch=20, col=\"green\",cex=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "res <- read.csv(\"./DEG/diesel_output.csv\", header=TRUE, row.names=1)#Youll need to add in some data for this to work!\n",
    "\n",
    "# Make a basic volcano plot\n",
    "with(res, plot(log2FoldChange, -log10(padj), pch=20, \n",
    "               main=\"Diesel Exhaust vs Standard Laboratory Atmosphere\", \n",
    "               xlim=c(-3,3), \n",
    "               ylim=c(0,40),cex=0.5))#change xlim or ylim to frame it out. cex is dot size  \n",
    "\n",
    "# Add colored points: red if padj<0.05, orange of log2FC>1, green if both)\n",
    "with(subset(res, padj<.05 ), points(log2FoldChange, -log10(padj), pch=20, col=\"red\",cex=0.5))\n",
    "with(subset(res, abs(log2FoldChange)>1), points(log2FoldChange, -log10(padj), pch=20, col=\"orange\",cex=0.5))\n",
    "with(subset(res, padj<.05 & abs(log2FoldChange)>1), points(log2FoldChange, -log10(padj), pch=20, col=\"green\",cex=0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global heat maps\n",
    "Global heat maps can be handy to look at the expression across multiple samples. At a quick glance, this will help you look at the raw data and cluster the samples out differently. These usually are based upon the hit count data, and will often include the controls for scaling. \n",
    "\n",
    "In this case we are using the heatmap.2 function as we did above, but instead of plotting out the distance matrix we will be plotting the hit counts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "data <- read.csv('./DEG/full_counts.csv', header=T, row.names=1)#make sure this data is just Gene in the first column followed by the columns of hitcounts\n",
    "heatmap.2(as.matrix(top_n(data,500)), #selects just the columns of hit counts from the file, takes just the top 500 genes in this case. \n",
    "          scale=c('row'), #the direction to scale the information. For \"row\" this will more represent the differences across samples\n",
    "          labRow='', \n",
    "          trace='none',\n",
    "          col='bluered',#Changes the color to make it pretty\n",
    "          Colv=TRUE, #Performs a standard hierarchal clustering for the columns (samples)\n",
    "          Rowv=TRUE, #Likewise performs it for the rows (genes)\n",
    "          cexRow=.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Focused heat maps\n",
    "Usually you have a few genes of interest that you want to dial in on as you get further in to the analysis. This can be things like \"I want to see how Actb is upregulated in my Drug X treated sample compared to control, and how its downregulated in my Drug Y treated sample compared to control\". These can also be done similarly to the global heat maps above, or you can utilize other packages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Venn diagrams\n",
    "Venn diagrams are a great way of comparing and contrasting samples as well. Often times you might think \"what genes are common or unique to my samples\". What better way to visualize this than with a venn diagram showing the overlap and what is unique. \n",
    "\n",
    "Here we are going to use a bit of python for fun. We will create a quick function to identify the significant genes for the samples. These lists of genes can then be fed directly into a matplot library called [matplotlib_venn](https://pypi.org/project/matplotlib-venn/) for plotting. Like the bash and R, there will be full python courses offered for anyone interested. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_venn import venn2\n",
    "import csv, os\n",
    "\n",
    "def counter(input_sheet):\n",
    "    genes=[] #Empty list to populate later\n",
    "    with open(input_sheet) as chart:\n",
    "        reader=csv.DictReader(chart, delimiter=',') #Open the CSV file as a dictionary to parse the columns\n",
    "        for row in reader: #Search through each row to then\n",
    "           if str(row['padj']) != 'NA': #If it has a P value\n",
    "               if float(row['padj']) <0.05: #If its significant. You can also use: if float(row['padj']) < 0.05 and float(row['log2fc']) > 1: as example\n",
    "                   genes.append(row['']) #Add that significant gene to the list\n",
    "    chart.close()\n",
    "    return genes #Return the list of signficant genes.\n",
    "\n",
    "urban_gene_list=counter('./DEG/urban_output.csv')\n",
    "diesel_gene_list=counter('./DEG/diesel_output.csv')\n",
    "\n",
    "venn2([set(urban_gene_list), set(diesel_gene_list)], set_labels = ('Urban Atmosphere','Diesel Exhaust'))\n",
    "plt.title('Comparison of common or unique genes for each sample as compared to Standard Laboratory Air\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now see how there are some shared and some unique genes. From this we can start to infer that some genes are upregulated just in response to general pollutants, whereas others are more likely specific to pathways associated just with the unique features of the different pollutants. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gene ontology/Pathway analysis\n",
    "Having individual genes is great, but the next step is nearly always related to groups of genes and how they could be inter-related. For the major organisms like Human/mouse/rat, there are a plethora of tools available. Things like []() and []() are commonly purchased for large scale licenses, with both available here at Rutgers. Regardless, there are also [gene ontologies]() for a much wider berth of organisms, as well as custom gene ontologies that can be manually curated and broad pan-bacterial ontologies. Fortunately with *E.coli*, there are available ontologies. \n",
    "\n",
    "Most of these tools rely on enrichment analysis. Quite simply, based on the number of genes inputed there are an expected number of genes. If your list has signficantly more or less than anticipated (based on z-score), then it is enriched. There are major limitations to this as it does not include direction in many cases and is only as good as your input list, so be careful of cherry picking data. If you also dont have enough genes, or enough genes in connected ontologies, you wont get any data out. \n",
    "\n",
    "This will follow the vignette [found here](https://github.com/tanghaibao/goatools/blob/master/notebooks/goea_nbt3102.ipynb), so if you want extra details go there! Since they have a full annotation for much of it Ill include just the code here for brevity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from goatools.base import download_go_basic_obo\n",
    "obo_fname = download_go_basic_obo()\n",
    "from goatools.base import download_ncbi_associations\n",
    "fin_gene2go = download_ncbi_associations()\n",
    "from goatools.obo_parser import GODag\n",
    "obodag = GODag(\"go-basic.obo\")\n",
    "\n",
    "from goatools.anno.genetogo_reader import Gene2GoReader\n",
    "\n",
    "objanno = Gene2GoReader(fin_gene2go, taxids=[511145])#Taxa id for E.coli strain we are using\n",
    "ns2assoc = objanno.get_ns2assc()\n",
    "from gene_result import GENEID2NT as GeneID2nt_coli\n",
    "\n",
    "from goatools.goea.go_enrichment_ns import GOEnrichmentStudyNS\n",
    "goeaobj = GOEnrichmentStudyNS(\n",
    "        GeneID2nt_coli.keys(), # List of protein-coding genes\n",
    "        ns2assoc, # geneid/GO associations\n",
    "        obodag, # Ontologies\n",
    "        propagate_counts = False,\n",
    "        alpha = 0.05, # default significance cut-off\n",
    "        methods = ['fdr_bh']) # defult multipletest correction method\n",
    "\n",
    "geneids_urban_study=urban_gene_list\n",
    "\n",
    "goea_urban_results_all = goeaobj.run_study(geneids_urban_study, prt=None)\n",
    "goea_urban_results_sig = [r for r in goea_urban_results_all if r.p_fdr_bh < 0.05]\n",
    "\n",
    "goeaobj.wr_txt(\"urban_GO.txt\", goea_urban_results_sig)\n",
    "\n",
    "geneids_diesel_study=diesel_gene_list\n",
    "\n",
    "goea_diesel_results_all = goeaobj.run_study(geneids_diesel_study, prt=None)\n",
    "goea_diesel_results_sig = [r for r in goea_diesel_results_all if r.p_fdr_bh < 0.05]\n",
    "\n",
    "goeaobj.wr_txt(\"diesel_GO.txt\", goea_diesel_results_sig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now very unfortunately, this study does not have enough drastic alterations in gene expression to cause any major enrichment. So the output files here will be empty and it should show 0 pathways enriched. In that case you would have to more manually go through the data and identify trends. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Basic bar charts\n",
    "In most cases for gene ontologies, they are displayed as a basic bar chart. It is quite simply usually the y-axis as the ontology, and the x-axis as the number of genes enriched or -log(pvalue). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bubble plots\n",
    "Bubble plots are the new craze for this, but are a bit more advanced. Ill just mention them here, but you can then plot the p-value, z-score, number of gene ratios, and directionality if you can include it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional follow up for a [fun challenge](https://www.youtube.com/watch?v=dQw4w9WgXcQ)\n",
    "If anyone dove into the paper this raw data is from, you'll see that there is another set of samples. These are *E.coli* that were first adapted to the diesel atmosphere (and acrued a few mutations) prior to the RNA-seq experiment. Using the different steps learned above, try downloading this new data set and processing it to compare how the diesel adapted samples vary from the wild type diesel samples. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
